{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3t18N061UZ4"
      },
      "source": [
        "**Analyse morpho-syntaxique avec NLTK (https://www.nltk.org/)**\n",
        "Dans ce TP je vous propose de comprendre et ensuite de tester plusieurs fonctionnalités de NLTK et SpaCy pour le pre-traitement et la vectorisation de textes. Je vous donne un example pour chaque fonctionnalité, prenez le temps de tester avec des autres phrases, et comprendre comment manipuler ce type de données textuelles.\n",
        "\n",
        "**Tokenisation et POS** avec NLTK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZVIbEPn0rK2",
        "outputId": "d2877832-8da3-4bc0-9599-323c0098dfe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Today', 'NN'), ('is', 'VBZ'), ('raining', 'VBG'), ('!', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Skyzo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Skyzo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = nltk.word_tokenize(\"Today is raining!\")\n",
        "print(nltk.pos_tag(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUUdKKP1xtE"
      },
      "source": [
        "Découvrons quelles sont les étiquettes les plus courantes dans la catégorie NEWS du corpus Brown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGDDIDXn12U5",
        "outputId": "e9ed7de6-603e-406f-e109-f0d2d38973ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\Skyzo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     C:\\Users\\Skyzo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('NOUN', 30654),\n",
              " ('VERB', 14399),\n",
              " ('ADP', 12355),\n",
              " ('.', 11928),\n",
              " ('DET', 11389),\n",
              " ('ADJ', 6706),\n",
              " ('ADV', 3349),\n",
              " ('CONJ', 2717),\n",
              " ('PRON', 2535),\n",
              " ('PRT', 2264),\n",
              " ('NUM', 2166),\n",
              " ('X', 92)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "brown_news_tagged = brown.tagged_words(categories='news',tagset='universal')\n",
        "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
        "tag_fd.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C994RhP75nY7"
      },
      "source": [
        "**Stemming** avec NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEgo6sgZ5uZ5",
        "outputId": "2f4fd462-818e-4a6a-e043-eeeea95718e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "walk\n",
            "walk\n",
            "walk\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "for word in ['walking', 'walks', 'walked']:\n",
        "    print(porter.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTLT0I0o2TbH"
      },
      "source": [
        "**Distribution des mots dans le texte**\n",
        "\n",
        "La méthode text.similar() prend un mot w, recherche tous les contextes w1 w w2, puis tous les mots w’ qui apparaissent dans le même contexte, c.-à-d. w1 w’ w2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on-v-ycr2jmI",
        "outputId": "37f8bedd-bb7a-4723-c178-d641fbc170fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "made said done put had seen found given left heard was been brought\n",
            "set got that took in told felt\n"
          ]
        }
      ],
      "source": [
        "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
        "#text.similar('woman')\n",
        "#Testez les mots suivants et d'autres mots:\n",
        "text.similar('bought')\n",
        "#text.similar('the')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2adGMi7F27zl"
      },
      "source": [
        "**Commment créer une CFG?**\n",
        "\n",
        "Définissons une grammaire et voyons comment analyser une phrase simple admise par la grammaire.\n",
        "\n",
        "Quelles phrases peut reconnaître cette grammaire?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km_ImXJR3Pmj",
        "outputId": "3dc997df-a183-4a7c-c54d-7a1e2ff7e885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S (NP (N dogs)) (VP (V chase) (NP (N cats))))\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import treebank\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP | V NP PP\n",
        "PP -> P NP\n",
        "V -> 'saw' | 'ate' | 'walked' | 'chase'\n",
        "NP -> 'John' | 'Mary' | 'Bob' | Det N | Det N PP | N\n",
        "Det -> 'a' | 'an' | 'the' | 'my'\n",
        "N -> 'man' | 'dog' | 'cat' | 'telescope' | 'park'| 'dogs' | 'cats'\n",
        "P -> 'in' | 'on' | 'by' | 'with'\n",
        "\"\"\")\n",
        "#sent = \"Mary saw Bob\".split()\n",
        "sent = \"dogs chase cats\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
        "for tree in rd_parser.parse(sent):\n",
        "  print(tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soE3nJC38KeE"
      },
      "source": [
        "Modifiez la grammaire pour que elle puisse reconnaitre la phrase: \"dogs chase cats\". Testez avec NLTK!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hnu1-qwO8Vzr"
      },
      "outputs": [],
      "source": [
        "#Testez ici!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDt7m76r8elV"
      },
      "source": [
        "**Une CFG pour le Français.** Quelles phrases peut reconnaître cette grammaire?\n",
        "\n",
        "Modifiez la grammaire pour que elle puisse reconnaitre des autres phrases!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW851t8g8tUH",
        "outputId": "74443568-7df5-4545-a029-a489e06fcf49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S (SN (Art le) (Nom chien)) (SV (V dort)))\n"
          ]
        }
      ],
      "source": [
        "grammaire = nltk.CFG.fromstring(\"\"\"\n",
        "S -> SN SV\n",
        "SN -> Art Nom\n",
        "SV -> V SN | V\n",
        "Nom -> 'chien' | 'chat'\n",
        "Art -> 'le'\n",
        "V -> 'mange'\n",
        "V -> 'dort'\n",
        "\"\"\")\n",
        "sent = \"le chien dort\".split()\n",
        "rd_parser = nltk.RecursiveDescentParser(grammaire)\n",
        "for tree in rd_parser.parse(sent):\n",
        "  print(tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSRqfxaU-le4"
      },
      "source": [
        "Testons maintenant l'outil SpaCy, une autre bibliothèque open-source pour le traitement avancé du langage naturel en Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2cOQ5YwCvY58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py\", line 13, in <module>\n",
            "    from . import pipeline  # noqa: F401\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipeline\\__init__.py\", line 1, in <module>\n",
            "    from .attributeruler import AttributeRuler\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipeline\\attributeruler.py\", line 8, in <module>\n",
            "    from ..language import Language\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\language.py\", line 46, in <module>\n",
            "    from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\pipe_analysis.py\", line 6, in <module>\n",
            "    from .tokens import Doc, Span, Token\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\tokens\\__init__.py\", line 1, in <module>\n",
            "    from ._serialize import DocBin\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\tokens\\_serialize.py\", line 14, in <module>\n",
            "    from ..vocab import Vocab\n",
            "  File \"spacy\\vocab.pyx\", line 1, in init spacy.vocab\n",
            "  File \"spacy\\tokens\\doc.pyx\", line 49, in init spacy.tokens.doc\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\schemas.py\", line 195, in <module>\n",
            "    class TokenPatternString(BaseModel):\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\main.py\", line 287, in __new__\n",
            "    cls.__try_update_forward_refs__()\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\main.py\", line 808, in __try_update_forward_refs__\n",
            "    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\typing.py\", line 553, in update_model_forward_refs\n",
            "    update_field_forward_refs(f, globalns=globalns, localns=localns)\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\typing.py\", line 528, in update_field_forward_refs\n",
            "    update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\typing.py\", line 519, in update_field_forward_refs\n",
            "    field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Skyzo\\AppData\\Roaming\\Python\\Python312\\site-packages\\pydantic\\typing.py\", line 65, in evaluate_forwardref\n",
            "    return cast(Any, type_)._evaluate(globalns, localns, set())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CF9Vz85ps5FA"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello     World!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('Hello     World!')\n",
        "for token in doc:\n",
        "    print('\"' + token.text + '\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heZQU1fEvnUL"
      },
      "outputs": [],
      "source": [
        "# détection de phrases\n",
        "\n",
        "doc = nlp(\"These are apples. These are oranges.\")\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-jw7OScvsDD"
      },
      "outputs": [],
      "source": [
        "# POS Tagging\n",
        "\n",
        "doc = nlp(\"Next week I'll be in Madrid.\")\n",
        "print([(token.text, token.tag_) for token in doc])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV23hKfpvsBE"
      },
      "outputs": [],
      "source": [
        "# NER Named Entity Recognition\n",
        "\n",
        "doc = nlp(u\"Next week I'll be in Madrid.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jMvAiWtvr-8"
      },
      "outputs": [],
      "source": [
        "# Spacy Entity Types\n",
        "\n",
        "doc = nlp(u\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5TG0AV7vr7c"
      },
      "outputs": [],
      "source": [
        "# displaCy\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(u'I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrN2aHz2vr4P"
      },
      "outputs": [],
      "source": [
        "# Chunking\n",
        "\n",
        "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.label_, chunk.root.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DW-AFGiwm2L"
      },
      "outputs": [],
      "source": [
        "# Dependency Parsing\n",
        "\n",
        "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
        "\n",
        "for token in doc:\n",
        "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
        "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNnF0IUKwqmR"
      },
      "outputs": [],
      "source": [
        "# Visualisation des Dependency Parsing\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(u'Wall Street Journal just published an interesting piece on crypto currencies')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiHaVsU_w0Am"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EnZKPCQwz8p"
      },
      "outputs": [],
      "source": [
        "# Load the en_core_web_lg embeddings\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvaC9K0kxkGE"
      },
      "outputs": [],
      "source": [
        "# View vector representation for the word 'banana'\n",
        "\n",
        "print(nlp.vocab[u'banana'].vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN0BORcNwz56"
      },
      "outputs": [],
      "source": [
        "# Word embedding Math: \"queen\" = \"king\"\n",
        "\n",
        "from scipy import spatial\n",
        "\n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "man = nlp.vocab[u'man'].vector\n",
        "woman = nlp.vocab[u'woman'].vector\n",
        "queen = nlp.vocab[u'queen'].vector\n",
        "king = nlp.vocab[u'king'].vector\n",
        "\n",
        "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
        "maybe_king = man - woman + queen\n",
        "computed_similarities = []\n",
        "\n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors\n",
        "    if not word.has_vector:\n",
        "        continue\n",
        "\n",
        "    similarity = cosine_similarity(maybe_king, word.vector)\n",
        "    computed_similarities.append((word, similarity))\n",
        "\n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "print([w[0].text for w in computed_similarities[:10]])\n",
        "\n",
        "# ['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'KINGS', 'kings', 'Kings']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgzWmQW3wz3m"
      },
      "outputs": [],
      "source": [
        "# Computing Similiarity\n",
        "\n",
        "banana = nlp.vocab[u'banana']\n",
        "dog = nlp.vocab[u'dog']\n",
        "fruit = nlp.vocab[u'fruit']\n",
        "animal = nlp.vocab[u'animal']\n",
        "\n",
        "print(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845\n",
        "print(banana.similarity(fruit), banana.similarity(animal)) # 0.67148364 0.2427285\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRWOetg-wz0N"
      },
      "outputs": [],
      "source": [
        "# Computing Similarity on entire texts\n",
        "\n",
        "target = nlp(u\"Cats are beautiful animals.\")\n",
        "\n",
        "doc1 = nlp(u\"Dogs are awesome.\")\n",
        "doc2 = nlp(u\"Some gorgeous creatures are felines.\")\n",
        "doc3 = nlp(u\"Dolphins are swimming mammals.\")\n",
        "\n",
        "print(target.similarity(doc1))  # 0.8901765218466683\n",
        "print(target.similarity(doc2))  # 0.9115828449161616\n",
        "print(target.similarity(doc3))  # 0.7822956752876101"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
